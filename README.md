# distributed-event-deduplication

### Sections:
1. Problem
2. Assumptions
3. Architecture
4. Deduplication strategy
5. Failure handling
6. Consistency guarantees
7. Scaling
8. Testing

## 4. Deduplication strategy
To achieve deduplication of events across multiple listener instances.
This definition allows the system to safely drop duplicate events
while guaranteeing a single logical processing per unique event.

#### Event Identity
Each event contains a unique `event_id` generated by the producer.
Two events are considered duplicates if they share the same `event_id`,
regardless of arrival time or listener instance.

# working:
Receive event
   |
Check Dedup Store (Redis)
   |
If not exists:
   Acquire Lock
      |
   Process Event
      |
   Save event_id in Dedup Store
      |
   Release Lock


Message Broker (Pub/Sub)
→ “Decouples producers and consumers for async, scalable communication.”

Distributed Lock
→ “Ensures only one instance processes a critical section in a distributed system.”

Dedup Store
→ “Persists processed identifiers to guarantee idempotency and avoid duplicates.”

Event comes (via Broker)
        |
   Dedup Store check
        |
   Distributed Lock
        |
   Process
        |
   Save in Dedup Store


#### We will use:
Redis Pub/Sub → message broker
Redis SETNX → distributed lock
Redis SET / TTL keys → dedup store